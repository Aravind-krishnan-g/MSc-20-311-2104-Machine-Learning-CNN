{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION OF CNN ON CIFAR-10 DATA SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSC. COMPUTER SCIENCE, IIITM-K, KERALA, INDIA\n",
    "<br>\n",
    "PROJECT GROUP MEMBERS:\n",
    "\n",
    "|NAME|DEPARTMENT|\n",
    "|----|----------|\n",
    "|GAYATHRI V|DATA ANALYTICS|\n",
    "|GAYATHRI PRASAD B|DATA ANALYTICS|\n",
    "|G ARAVIND KRISHNAN|DATA ANALYTICS|\n",
    "<br>[MEDIUM ARTICLE LINK](https://aravind-krishnan.medium.com/introduction-to-cnn-through-cifar-10-dataset-e70f67b158f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEPS:\n",
    "1. Importing libraries\n",
    "2. Data collection and normalization\n",
    "3. Model creation\n",
    "4. Model training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use tensorflow and its module [keras](https://www.tensorflow.org/api_docs/python/tf/keras), to create our CNN model. Different models of CNN are available in keras. We will use 'sequential' model which is simple and easy to implement. Next step is to import various layers that we use to create our model.\n",
    "<br> They are:\n",
    "1. [Conv2D layer](https://keras.io/api/layers/convolution_layers/convolution2d/): convolutional layer that will use.\n",
    "2. [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/): pooling layer with type of pooling as 'max'.\n",
    "3. [Dropout](https://keras.io/api/layers/regularization_layers/dropout/): it is a regularization layer.\n",
    "4. [LeakyReLu](https://keras.io/api/layers/activation_layers/leaky_relu/): It is an activation function modelled as a layer in keras.\n",
    "5. [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/): Flatten will convert multidimensional arrays to one dimensional ones.\n",
    "6. [Dense layer](https://keras.io/api/layers/core_layers/dense/) : fully connected layer in keras.\n",
    "<br> Finally, we will import ['to_categorical'](https://keras.io/api/utils/python_utils/#to_categorical-function), which is an encoder that will convert integer values to binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # CNN model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, LeakyReLU, Flatten, Dense  # layers that we use\n",
    "from tensorflow.keras.utils import to_categorical # to_categorical is to encode integer values to binary values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data collecting and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'keras.datasets' to collect our data ([CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)). Since we get the data in the required format, no data cleaning or preprocessing is required. After that, we will do normalization on the collected data. The data that we are dealing with is image (RGB) data, which implies the data values will range from 0 to 255. Dividing the values by 255 (maximum value) and subtracting a standard deviation of 0.5 will normalize the x values. Labels or y values should be divided by number of classes (10) for normalizing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data collected : <class 'numpy.ndarray'>\n",
      "Size and shape of input data for testing    :  (50000, 32, 32, 3)\n",
      "Size and shape of label data for testing    :  (50000, 1)\n",
      "Size and shape of input data for validation :  (10000, 32, 32, 3)\n",
      "Size and shape of label data for validation :  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# collecting data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(\"Type of data collected :\",type(x_train))\n",
    "print('Size and shape of input data for testing    : ',x_train.shape) # first value indicate the size of data set \n",
    "print('Size and shape of label data for testing    : ',y_train.shape) # and rest will be its size\n",
    "print('Size and shape of input data for validation : ',x_test.shape)\n",
    "print('Size and shape of label data for validation : ',y_test.shape)\n",
    "\n",
    "# normalizing\n",
    "x_train = (x_train/255) - 0.5\n",
    "x_test  = (x_test/255) - 0.5\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test  = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we will use sequential model for our CNN. After creating an object of the model, we will start adding each layer one by one specifying parameters in each layer. Regularization and activation layers are added in between. Once we compete adding layers, model.compile() function is called, mentioning optimizer(in our case 'adam'), loss ('categorical_crossentropy') and metrics ('accuracy') parameters. Running the code will create our model of CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 231,338\n",
      "Trainable params: 231,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = Sequential() # creating a model object\n",
    "\n",
    "# adding layers onto the model 'classifier'\n",
    "classifier.add(Conv2D(16, (3, 3), padding = 'same', activation = 'relu', input_shape = (32, 32, 3))) # no of filters is 16\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2), padding = 'valid'))\n",
    "\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu')) # no of filters is 32\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2), padding = 'valid'))\n",
    "\n",
    "classifier.add(Dropout(0.25))  # regularization layer\n",
    "classifier.add(LeakyReLU(0.2)) # activation layer\n",
    "\n",
    "classifier.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu')) # no of filters is 64\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2), padding = 'valid'))\n",
    "\n",
    "classifier.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu')) # no of filters is 128\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2), padding = 'valid'))\n",
    "\n",
    "classifier.add(Dropout(0.25))  # regularization layer\n",
    "classifier.add(LeakyReLU(0.2)) # activation layer\n",
    "\n",
    "classifier.add(Flatten()) # converting to one dimensional data\n",
    "\n",
    "classifier.add(Dense(256)) # fully connected layer \n",
    "classifier.add(Dense(10, activation='sigmoid')) # activation function for classification is 'sigmoid' \n",
    " \n",
    "# compile will connect all layers that we added and complete our model\n",
    "classifier.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "classifier.summary() # creates a model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once model create is complete, next step is to train our model with training data and test it with validation data set. x_train and y_train will be our training data and x_test and y_test will be our validation data. We will run the training process upto 10 epochs. Training and validaton is done by calling the model.fit() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 31s 19ms/step - loss: 1.7123 - accuracy: 0.3642 - val_loss: 1.2452 - val_accuracy: 0.5534\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.1736 - accuracy: 0.5770 - val_loss: 1.0052 - val_accuracy: 0.6418\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0144 - accuracy: 0.6371 - val_loss: 0.9169 - val_accuracy: 0.6788\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9227 - accuracy: 0.6756 - val_loss: 0.8326 - val_accuracy: 0.7072\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8594 - accuracy: 0.6958 - val_loss: 0.8274 - val_accuracy: 0.7135\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.8063 - accuracy: 0.7157 - val_loss: 0.8143 - val_accuracy: 0.7159\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7821 - accuracy: 0.7260 - val_loss: 0.7876 - val_accuracy: 0.7271\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7456 - accuracy: 0.7363 - val_loss: 0.7493 - val_accuracy: 0.7453\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7246 - accuracy: 0.7430 - val_loss: 0.7378 - val_accuracy: 0.7456\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7061 - accuracy: 0.7509 - val_loss: 0.7411 - val_accuracy: 0.7441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b9b8076a90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  epochs=10,\n",
    "  validation_data=(x_test,  y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that validation accuracy (val_accuracy) will vary each time you run the code. Most of the time, value obtained was above 70% with peak value of 75%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
